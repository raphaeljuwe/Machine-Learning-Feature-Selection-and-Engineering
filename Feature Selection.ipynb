{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-aa0d82aa2a53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write function for getting data from yahoo finance given a ticker, start date, and end date\n",
    "def get_ticker_returns_df(ticker, start_date, end_date):\n",
    "    \n",
    "    ticker_data = pdr.DataReader(ticker, start = '2017-12-01', end = '2020-01-31', data_source = 'yahoo')\n",
    "\n",
    "    ticker_close_data = ticker_data['Adj Close']\n",
    "\n",
    "    ticker_close_df = pd.DataFrame(data = ticker_close_data)\n",
    "    ticker_close_df.columns = [column.replace(' ', '_') for column in ticker_close_df.columns]\n",
    "    \n",
    "    ticker_close_df['CurrDay_'+ticker+'_Return'] = ticker_close_df['Adj_Close'].pct_change()\n",
    "    ticker_close_df['PrevDay_Adj_Close'] = ticker_close_df.Adj_Close.shift(1)\n",
    "    ticker_close_df['PrevDay_'+ticker+'_Return'] = ticker_close_df['PrevDay_Adj_Close'].pct_change()\n",
    "    ticker_close_df['5PrevDays_'+ticker+'_Return'] = ticker_close_df['PrevDay_'+ticker+'_Return'].rolling(window = 5, min_periods = 5).mean()\n",
    "    ticker_close_df['10PrevDays_'+ticker+'_Return'] = ticker_close_df['PrevDay_'+ticker+'_Return'].rolling(window = 10, min_periods = 10).mean()\n",
    "    \n",
    "    ticker_close_df = ticker_close_df.reset_index()\n",
    "    ticker_close_df = ticker_close_df[ticker_close_df.Date >= start_date]\n",
    "    ticker_close_df = ticker_close_df[ticker_close_df.Date <= end_date]\n",
    "    ticker_close_df = ticker_close_df.dropna()\n",
    "    \n",
    "    ticker_returns_df = ticker_close_df.reset_index()\n",
    "    ticker_returns_df = ticker_returns_df.drop(['index','Adj_Close','PrevDay_Adj_Close'], axis=1)\n",
    "    \n",
    "    return ticker_returns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape table data off of wikipedia to get sector classifications of S&P500 stocks\n",
    "sp500_qualitative_data = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "\n",
    "sp500_qualitative_df = sp500_qualitative_data[0]\n",
    "sp500_qualitative_df = sp500_qualitative_df.drop(['Security','SEC filings','GICS Sub Industry','Headquarters Location','Date first added','CIK','Founded'], axis=1)\n",
    "sp500_qualitative_df = sp500_qualitative_df.rename(columns = {'GICS Sector': 'Sector'})\n",
    "sp500_qualitative_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect previous day returns and 5/10 day moving average previous day returns for each symbol for the time period\n",
    "all_ticker_returns_df = pd.DataFrame()\n",
    "\n",
    "for ticker in sp500_qualitative_df.Symbol:\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        ticker_returns_df = get_ticker_returns_df(ticker,'2018-01-01','2019-12-31')\n",
    "        ticker_returns_df.columns = [column.replace('_'+ticker+'_', '_') for column in ticker_returns_df.columns]\n",
    "        ticker_returns_df['Symbol'] = ticker\n",
    "\n",
    "        new_df = pd.merge(sp500_qualitative_df, ticker_returns_df, how='inner', on=['Symbol'])\n",
    "        \n",
    "        all_ticker_returns_df = all_ticker_returns_df.append(new_df, ignore_index = True)\n",
    "        \n",
    "        print('Got returns for: ' + ticker)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print('Cannot get returns for: ' + ticker)\n",
    "        \n",
    "        continue\n",
    "        \n",
    "all_ticker_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate data on sector and date to get sector previous day return and 5/10 day moving average previous day returns\n",
    "merge_df1 = pd.merge(all_ticker_returns_df, all_ticker_returns_df.groupby(['Sector', 'Date'], as_index = False)['PrevDay_Return'].mean().rename(columns = {'PrevDay_Return': 'PrevDay_Sector_Return'}), how='inner', on=['Sector','Date'])\n",
    "merge_df2 = pd.merge(merge_df1, all_ticker_returns_df.groupby(['Sector', 'Date'], as_index = False)['5PrevDays_Return'].mean().rename(columns = {'5PrevDays_Return': '5PrevDays_Sector_Return'}), how='inner', on=['Sector','Date'])\n",
    "all_ticker_sector_returns_df = pd.merge(merge_df2, all_ticker_returns_df.groupby(['Sector', 'Date'], as_index = False)['10PrevDays_Return'].mean().rename(columns = {'10PrevDays_Return': '10PrevDays_Sector_Return'}), how='inner', on=['Sector','Date'])\n",
    "all_ticker_sector_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get S&P500 previous day return and 5/10 day moving average previous day returns for the time frame\n",
    "SP500_returns_df = get_ticker_returns_df('^GSPC','2018-01-01','2019-12-31')\n",
    "SP500_returns_df = SP500_returns_df.drop(['CurrDay_^GSPC_Return'], axis=1)\n",
    "SP500_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get VIX previous day return and 5/10 day moving average previous day returns for the time frame\n",
    "VIX_returns_df = get_ticker_returns_df('^VIX','2018-01-01','2019-12-31')\n",
    "VIX_returns_df = VIX_returns_df.drop(['CurrDay_^VIX_Return'], axis=1)\n",
    "VIX_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all data into one dataframe to prepare for dimensionality reduction\n",
    "merge_another_df1 = pd.merge(all_ticker_sector_returns_df, SP500_returns_df, how='inner', on=['Date'])\n",
    "all_data_df = pd.merge(merge_another_df1, VIX_returns_df, how='inner', on=['Date'])\n",
    "all_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale all data and apply pca for four principal components for each symbol and store results in dictionaries\n",
    "train_df = all_data_df[all_data_df.Date < '2019-10-01']\n",
    "test_df = all_data_df[all_data_df.Date >= '2019-10-01']\n",
    "\n",
    "train_dict = {}\n",
    "test_dict = {}\n",
    "explained_variance_ratio_dict = {}\n",
    "\n",
    "for symbol in test_df.Symbol.unique():\n",
    "\n",
    "    ticker_train_df = train_df[train_df.Symbol == symbol].reset_index().drop(['index'], axis=1)\n",
    "    ticker_test_df = test_df[test_df.Symbol == symbol].reset_index().drop(['index'], axis=1)\n",
    "\n",
    "    train_reference_target_variables_df = ticker_train_df.iloc[:, 0:4]\n",
    "    test_reference_target_variables_df = ticker_test_df.iloc[:, 0:4]\n",
    "\n",
    "    train_explanatory_variables_df = ticker_train_df.iloc[:, 4:16]\n",
    "    test_explanatory_variables_df = ticker_test_df.iloc[:, 4:16]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_explanatory_variables_df)\n",
    "\n",
    "    train_explanatory_variables_scaled_df = scaler.transform(train_explanatory_variables_df)\n",
    "    test_explanatory_variables_scaled_df = scaler.transform(test_explanatory_variables_df)\n",
    "\n",
    "    pca = PCA(n_components = 4)\n",
    "    pca.fit(train_explanatory_variables_scaled_df)\n",
    "\n",
    "    explained_variance_ratio_dict[symbol] = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "    train_pca = pca.transform(train_explanatory_variables_scaled_df)\n",
    "    train_pca_df = pd.DataFrame(data = train_pca, columns = ['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "    train_all_reduced_df = pd.merge(train_reference_target_variables_df, train_pca_df, left_index=True, right_index=True)\n",
    "    \n",
    "    train_dict[symbol] = train_all_reduced_df\n",
    "\n",
    "    test_pca = pca.transform(test_explanatory_variables_scaled_df)\n",
    "    test_pca_df = pd.DataFrame(data = test_pca, columns = ['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "    test_all_reduced_df = pd.merge(test_reference_target_variables_df, test_pca_df, left_index=True, right_index=True)\n",
    "    \n",
    "    test_dict[symbol] = test_all_reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_max = max(explained_variance_ratio_dict.keys(), key=(lambda k: explained_variance_ratio_dict[k]))\n",
    "key_min = min(explained_variance_ratio_dict.keys(), key=(lambda k: explained_variance_ratio_dict[k]))\n",
    "\n",
    "print('Maximum Explained Variance - ', key_max, ': ', explained_variance_ratio_dict[key_max])\n",
    "print('Minimum Explained Variance - ', key_min, ': ', explained_variance_ratio_dict[key_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict['MSFT'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['MSFT'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in test_df.Symbol.unique():\n",
    "    \n",
    "    iterating_train_df = train_dict[symbol]\n",
    "    iterating_test_df = test_dict[symbol]\n",
    "\n",
    "    threshold = iterating_test_df['CurrDay_Return'].std()\n",
    "\n",
    "    iterating_train_df['True_Label'] = '0'\n",
    "    iterating_train_df.loc[iterating_train_df.CurrDay_Return > 1 * threshold, 'True_Label'] = '1'\n",
    "    iterating_train_df.loc[iterating_train_df.CurrDay_Return < -1 * threshold, 'True_Label'] = '-1'\n",
    "    \n",
    "    iterating_train_df['OneHot_True_Label_PosOne'] = '0'\n",
    "    iterating_train_df.loc[iterating_train_df.True_Label == '1', 'OneHot_True_Label_PosOne'] = '1'\n",
    "    iterating_train_df['OneHot_True_Label_Zero'] = '0'\n",
    "    iterating_train_df.loc[iterating_train_df.True_Label == '0', 'OneHot_True_Label_Zero'] = '1'\n",
    "    iterating_train_df['OneHot_True_Label_NegOne'] = '0'\n",
    "    iterating_train_df.loc[iterating_train_df.True_Label == '-1', 'OneHot_True_Label_NegOne'] = '1'\n",
    "\n",
    "    iterating_test_df['True_Label'] = '0'\n",
    "    iterating_test_df.loc[iterating_test_df.CurrDay_Return > threshold, 'True_Label'] = '1'\n",
    "    iterating_test_df.loc[iterating_test_df.CurrDay_Return < -1 * threshold, 'True_Label'] = '-1'\n",
    "    \n",
    "    iterating_test_df['OneHot_True_Label_PosOne'] = '0'\n",
    "    iterating_test_df.loc[iterating_test_df.True_Label == '1', 'OneHot_True_Label_PosOne'] = '1'\n",
    "    iterating_test_df['OneHot_True_Label_Zero'] = '0'\n",
    "    iterating_test_df.loc[iterating_test_df.True_Label == '0', 'OneHot_True_Label_Zero'] = '1'\n",
    "    iterating_test_df['OneHot_True_Label_NegOne'] = '0'\n",
    "    iterating_test_df.loc[iterating_test_df.True_Label == '-1', 'OneHot_True_Label_NegOne'] = '1'\n",
    "    \n",
    "    train_dict[symbol] = iterating_train_df\n",
    "    test_dict[symbol] = iterating_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict['MSFT'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['MSFT'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_variables = {}\n",
    "test_input_variables = {}\n",
    "train_output_variables = {}\n",
    "test_output_variables = {}\n",
    "\n",
    "for symbol in test_df.Symbol.unique():\n",
    "    \n",
    "    train_input_variables[symbol] = train_dict[symbol].iloc[:, 4:8]\n",
    "    test_input_variables[symbol] = test_dict[symbol].iloc[:, 4:8]\n",
    "    \n",
    "    train_output_variables[symbol] = train_dict[symbol].iloc[:, 9:12]\n",
    "    test_output_variables[symbol] = test_dict[symbol].iloc[:, 9:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_variables['MSFT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output_variables['MSFT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(session, train_x, train_y):\n",
    "    \n",
    "    session.run(init_op)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_batch = int(train_x.shape[0] / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = train_y[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            session.run(training_op, feed_dict={x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(session, split_size = 10):\n",
    "    \n",
    "    results = []\n",
    "    kf = KFold(n_splits = split_size)\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(train_input_variables_df, train_output_variables_df):\n",
    "        \n",
    "        train_x = train_input_variables_df[train_input_variables_df.index.isin(train_idx)]\n",
    "        train_y = train_output_variables_df[train_output_variables_df.index.isin(train_idx)]\n",
    "        \n",
    "        val_x = train_input_variables_df[train_input_variables_df.index.isin(val_idx)]\n",
    "        val_y = train_output_variables_df[train_output_variables_df.index.isin(val_idx)]\n",
    "        \n",
    "        run_train(session, train_x, train_y)\n",
    "        \n",
    "        results.append(session.run(accuracy, feed_dict={x: val_x, y: val_y}))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_accuracies = {}\n",
    "train_accuracies = {}\n",
    "test_accuracies = {}\n",
    "train_dict_with_predictions = {}\n",
    "test_dict_with_predictions = {}\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "for symbol in test_df.Symbol.unique():\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        train_input_variables_df = train_input_variables[symbol]\n",
    "        test_input_variables_df = test_input_variables[symbol]\n",
    "\n",
    "        train_output_variables_df = train_output_variables[symbol]\n",
    "        test_output_variables_df = test_output_variables[symbol]\n",
    "\n",
    "        n_inputs = 4\n",
    "        n_hidden_1 = 60\n",
    "        n_outputs = 3\n",
    "        n_sample = train_input_variables_df.shape[0]\n",
    "\n",
    "        n_epochs = 2000\n",
    "        n_batches = 10\n",
    "        batch_size = int(train_input_variables_df.shape[0] / n_batches)\n",
    "\n",
    "        with tf.Graph().as_default() as graph:\n",
    "\n",
    "            with tf.name_scope(\"Inputs\"):    \n",
    "                x = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"x\")\n",
    "                y = tf.placeholder(tf.float32, shape=(None, n_outputs), name=\"y\")\n",
    "\n",
    "            with tf.name_scope(\"output\"):\n",
    "                fc1    = tf.layers.dense(x, n_hidden_1, activation = tf.nn.relu, name=\"fc1\")\n",
    "                logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "                Y_prob = tf.nn.softmax(logits, name=\"Y_prob\")\n",
    "\n",
    "            with tf.name_scope(\"train\"):\n",
    "                xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y, name=\"xentropy\")\n",
    "                loss = tf.reduce_mean(xentropy, name='loss')\n",
    "                optimizer = tf.train.AdamOptimizer()\n",
    "                training_op = optimizer.minimize(loss)\n",
    "\n",
    "            with tf.name_scope(\"eval\"):\n",
    "                correct = tf.equal(tf.argmax(logits,axis=1), tf.argmax(y,axis=1))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "            with tf.name_scope(\"init\"):\n",
    "                init_op = tf.global_variables_initializer()\n",
    "\n",
    "            with tf.Session() as session:\n",
    "\n",
    "                cross_validation_results = cross_validate(session)\n",
    "                cross_validation_accuracy = sum(cross_validation_results) / len(cross_validation_results)\n",
    "\n",
    "                train_accuracy = session.run(accuracy, feed_dict={x: train_input_variables_df, y: train_output_variables_df})\n",
    "                test_accuracy = session.run(accuracy, feed_dict={x: test_input_variables_df, y: test_output_variables_df})\n",
    "\n",
    "                train_probs = session.run(Y_prob, feed_dict={x: train_input_variables_df, y: train_output_variables_df})\n",
    "                test_probs = session.run(Y_prob, feed_dict={x: test_input_variables_df, y: test_output_variables_df})\n",
    "\n",
    "        cross_validation_accuracies[symbol] = cross_validation_accuracy\n",
    "        train_accuracies[symbol] = train_accuracy\n",
    "        test_accuracies[symbol] = test_accuracy        \n",
    "\n",
    "        train_label = np.argmax(train_probs, axis=1)\n",
    "        train_label_df = pd.DataFrame(data = train_label, columns = ['Modeled_Label'])\n",
    "\n",
    "        train_label_df['Predicted_Label'] = ''\n",
    "        train_label_df.loc[train_label_df.Modeled_Label == 0, 'Predicted_Label'] = '1'\n",
    "        train_label_df.loc[train_label_df.Modeled_Label == 1, 'Predicted_Label'] = '0'\n",
    "        train_label_df.loc[train_label_df.Modeled_Label == 2, 'Predicted_Label'] = '-1'\n",
    "\n",
    "        train_label_df.drop(['Modeled_Label'], axis = 1, inplace = True)\n",
    "\n",
    "        test_label = np.argmax(test_probs, axis = 1)\n",
    "        test_label_df = pd.DataFrame(data = test_label, columns = ['Modeled_Label'])\n",
    "\n",
    "        test_label_df['Predicted_Label'] = ''\n",
    "        test_label_df.loc[test_label_df.Modeled_Label == 0, 'Predicted_Label'] = '1'\n",
    "        test_label_df.loc[test_label_df.Modeled_Label == 1, 'Predicted_Label'] = '0'\n",
    "        test_label_df.loc[test_label_df.Modeled_Label == 2, 'Predicted_Label'] = '-1'\n",
    "\n",
    "        test_label_df.drop(['Modeled_Label'], axis = 1, inplace = True)\n",
    "\n",
    "        train_dict_with_predictions_df = pd.merge(train_dict[symbol], train_label_df, left_index=True, right_index=True)\n",
    "        test_dict_with_predictions_df = pd.merge(test_dict[symbol], test_label_df, left_index=True, right_index=True)\n",
    "\n",
    "        train_dict_with_predictions_df['Matching_Prediction_Flag'] = '0'\n",
    "        train_dict_with_predictions_df.loc[train_dict_with_predictions_df.True_Label == train_dict_with_predictions_df.Predicted_Label, 'Matching_Prediction_Flag'] = '1'\n",
    "\n",
    "        test_dict_with_predictions_df['Matching_Prediction_Flag'] = '0'\n",
    "        test_dict_with_predictions_df.loc[test_dict_with_predictions_df.True_Label == test_dict_with_predictions_df.Predicted_Label, 'Matching_Prediction_Flag'] = '1'\n",
    "\n",
    "        train_dict_with_predictions_df['Opposite_Prediction_Flag'] = '0'\n",
    "        train_dict_with_predictions_df.loc[(train_dict_with_predictions_df.True_Label == '1') & (train_dict_with_predictions_df.Predicted_Label == '-1'), 'Opposite_Prediction_Flag'] = '1'\n",
    "        train_dict_with_predictions_df.loc[(train_dict_with_predictions_df.True_Label == '-1') & (train_dict_with_predictions_df.Predicted_Label == '1'), 'Opposite_Prediction_Flag'] = '1'\n",
    "\n",
    "        test_dict_with_predictions_df['Opposite_Prediction_Flag'] = '0'\n",
    "        test_dict_with_predictions_df.loc[(test_dict_with_predictions_df.True_Label == '1') & (test_dict_with_predictions_df.Predicted_Label == '-1'), 'Opposite_Prediction_Flag'] = '1'\n",
    "        test_dict_with_predictions_df.loc[(test_dict_with_predictions_df.True_Label == '-1') & (test_dict_with_predictions_df.Predicted_Label == '1'), 'Opposite_Prediction_Flag'] = '1'\n",
    "\n",
    "        train_dict_with_predictions[symbol] = train_dict_with_predictions_df\n",
    "        test_dict_with_predictions[symbol] = test_dict_with_predictions_df\n",
    "        \n",
    "        print('Successfully Modeled: ' + ticker)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print('UNSECCESSFULLY Modeled: ' + ticker)\n",
    "        \n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
